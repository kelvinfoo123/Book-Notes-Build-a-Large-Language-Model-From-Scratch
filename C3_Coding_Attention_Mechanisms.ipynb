{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **The problem with modeling long sequences**\n",
        "- Suppose we want to develop a language translation model to translate text from one language to another, a common approach is to use a deep neural network with an encoder and a decoder. The encoder first reads in and process the entire text, and the decoder produces the translated text.\n",
        "- Before transformers, recurrent neural networks were popular encoder-decoder architecture for language translation.\n",
        "- In an encoderâ€“decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden stateat each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder takes this final hidden state to generate the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for next-word prediction.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"800px\">\n",
        "\n",
        "- The limitation of encoder-decoder RNN is that the RNN cannot directly access earlier hidden states from the encoder during the decoding phase. Relying solely on the current hidden state can lead to a loss of context, especially in complex sentences where dependencies might span long distances.\n",
        "- Also, RNN must remember the entire encoded input in a single hidden state before passing it to the decoder.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"800px\">\n",
        "\n",
        "- Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component of modern LLM based on the transformer architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "nlRzHQZE_Xkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attending to different parts of the input with self-attention**\n",
        "- Self-attention computes attention weights by assessing and learning the relationships and dependencies between various parts of the input itself.\n",
        "- The input sequence, denotes as $x$, consists $T$ elements, $x(1), \\cdots, x(T)$. Each element of the sequence $x(i)$ corresponds to a d-dimensional embedding vector representing a specific token.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"600px\">\n",
        "\n",
        "- The goal of self-attention is to calculate context vectors $z(i)$ for each element $x(i)$ in the input sequence. Conext vectors create enriched representation of each element by incoporating information from all other elements in the sequence. For example, the context vector $z(2)$ is an embedding that contains information about $x(2)$ and all other input elements $x(1), \\cdots, x(T)$.\n",
        "- Self-attention first computes the intermediate $w$ known as attention scores. The intermediate attention score of $x(2)$ is computed by taking the dot product of the query $x(2)$ with every other input token.\n",
        "- Each of the attention score is then normalized so that attention weights sum up to 1. The softmax function is commonly used for normalization as it ensures that the attention weights are always positive.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"800px\">\n",
        "\n",
        "- The context vector $z(2)$ is the weighted sum of all input vectors, obtained by multiplying each input vector with its corresponding attention weights.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"800px\">"
      ],
      "metadata": {
        "id": "AIUV0w6VCPFp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYWEq_Yo-T6L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Arbitrary embeddings\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention score for x(2)\n",
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product\n",
        "\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXdlBuuPF1fW",
        "outputId": "a46c4f52-1549-49f7-d119-be0d8049406f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization using softmax\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGc0bxX-F85Y",
        "outputId": "88a4e664-c8c3-4fc4-839d-c23b03aff00a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute context vector z(2)\n",
        "query = inputs[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1GuJTIjGDAe",
        "outputId": "cd3d4c60-8d01-474b-86d9-109bd1f3f999"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The following code extends the above to compute context vector for $x(1), \\cdots, x(T)$ using matrix multiplication."
      ],
      "metadata": {
        "id": "6JPg-CiMG3ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_6D35S5Gqxx",
        "outputId": "0c75fe4d-2c7b-4e96-c2cf-4108d240205f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEm7z6PjHTuB",
        "outputId": "d08c8cc3-7e34-4a56-8114-d870a5211c11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing self-attention with trainable weights**\n",
        "- We now introduce scaled dot-product attention. We want to compute context vectors as weighted sums over the input vectors specific to a certain input element. The most notable difference to the above method is the introduction of weight matrices that are updated during model training.\n",
        "- We introduce the three trainable weight matrices $W_q, W_k, W_v$ that are used to project the embedding input tokens $x^{(i)}$ into query, key and value vectors respectively.\n",
        "- The query vector $q(2)$ is obtained via matrix multiplication between the input $x(2)$ and the weight matrix$ W_q$. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight matrices $W_k$ and $W_v$.\n",
        "- The attention score is computed with a dot product between $q(2)$ and the corresponding key vector. To scale the attention score, we divide by the square root of the embedding dimensions of the keys to obtain the attention weights.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"800px\">\n",
        "\n",
        "\n",
        "- The context vector $z(2)$ is computed as a weighted sum over the value vectors. The attention weights serve as a weighting factor that weighs the relative importance of each value vector.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"800px\">"
      ],
      "metadata": {
        "id": "8F2hwzK8Hab8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2\n",
        "\n",
        "# Initialize weight matrices\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "# Compute query, key and value vectors\n",
        "query_2 = x_2 @ W_query\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNE0YdDsHXf0",
        "outputId": "6c4ee535-85a7-4e0c-88ad-b22d2a7d7aca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1785, 0.5566])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention scores\n",
        "keys_2 = keys[1]\n",
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HLPozmEKcon",
        "outputId": "3eaaf737-06e1-4954-e205-e6a850c10d00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2005, 1.6979, 1.6871, 0.9065, 1.0177, 1.0741])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention weights\n",
        "d_k = keys.shape[1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)\n",
        "\n",
        "# Compute context vectors\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX1TeZX1K3WH",
        "outputId": "60a40d9a-71da-4016-d4f4-d3039bab1346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1553, 0.2208, 0.2191, 0.1262, 0.1365, 0.1421])\n",
            "tensor([0.4793, 0.9304])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de8VOcM-K-ot",
        "outputId": "965836b0-a284-4405-e6fe-0f267903016e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hiding future words with causal attention**\n",
        "- Causal attention restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores.\n",
        "- For each processed token, we mask out the future tokens, which come after the current token in the input text. We mask the attention weights above the diagonal with $-\\infty$, so that the softmax function treats them as zero probability, and we normalize the non-masked attention weights using softmax so that the attention weights sum to 1 in each row.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"800px\">\n",
        "\n"
      ],
      "metadata": {
        "id": "BSDYrAMULapV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fs5RJ_6LSLB",
        "outputId": "17983761-d9e5-4904-828b-ced0795192cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.9544, 1.4950,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.9422, 1.4754, 1.4570,   -inf,   -inf,   -inf],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937,   -inf,   -inf],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654,   -inf],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzfyKPX0QXvz",
        "outputId": "fd466837-4e2c-492a-a032-a9cd6c54cc14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4056, 0.5944, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2566, 0.3741, 0.3693, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2176, 0.2823, 0.2796, 0.2205, 0.0000, 0.0000],\n",
            "        [0.1826, 0.2178, 0.2191, 0.1689, 0.2115, 0.0000],\n",
            "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units.\n",
        "- Dropout can be applied after computing the attention weights.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"800px\">\n",
        "\n",
        "- In the following code, we applied a dropout rate of 50%, which means masking out half of the attention weights. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This ensures the average influence of the attention mechanism remains consistent during the training and inference phase."
      ],
      "metadata": {
        "id": "EiQ0-cqUQ9HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXZ2eBT8QvsP",
        "outputId": "a48d0948-5e1e-4d52-8e0c-cfb58f2c848e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.1888, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.7386, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4352, 0.5646, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4065, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5AnfH6KSHK1",
        "outputId": "3f0c30ef-7189-4ce6-ffaf-2b8c0e903484"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extending single-head attention to multi-head attention**\n",
        "- Multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights and then combining their outputs.\n",
        "- The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions."
      ],
      "metadata": {
        "id": "yxIh51gISUcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11MD_g0hSP02",
        "outputId": "b7de7b9b-2309-41cf-c089-f488c8ef6a0e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4F31cnSdTqDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}